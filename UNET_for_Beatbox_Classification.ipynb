{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ElBuberino/Tensorflow_Final_Project_Beatbox/blob/main/UNET_for_Beatbox_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPS5180mT6Jq"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/ElBuberino/Tensorflow_Final_Project_Beatbox"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "from glob import glob\n",
        "from IPython.display import Audio\n",
        "from scipy.io import wavfile\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPool2D, Conv2DTranspose, Concatenate, Input\n",
        "from tensorflow.keras.models import Model \n",
        "from numpy import newaxis\n",
        "\n",
        "\n",
        "# set path to the dataset\n",
        "PATH = \"/content/Tensorflow_Final_Project_Beatbox/Personal\"\n",
        "# set file ending\n",
        "CSV = \"*.csv\"\n",
        "WAV = \"*.wav\"\n",
        "# initiate two empty lists to save the inputs and labels in\n",
        "spec_list = []\n",
        "label_list = []\n",
        "\n",
        "# collect all csv files from the dataset\n",
        "all_csv_files = [file\n",
        "                 for path, subdir, files in os.walk(PATH)\n",
        "                 for file in glob(os.path.join(path, CSV))]\n",
        "\n",
        "# collect all wav files from the dataset\n",
        "all_wav_files = [file\n",
        "                 for path, subdir, files in os.walk(PATH)\n",
        "                 for file in glob(os.path.join(path, WAV))]\n",
        "\n",
        "# sort all files to assure that the right labels are assigned to the right inputs\n",
        "\n",
        "all_csv_files = sorted(all_csv_files)\n",
        "all_wav_files = sorted(all_wav_files)"
      ],
      "metadata": {
        "id": "6dYAnA6iUAs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the sorted csv list\n",
        "for x in range(10):\n",
        "  print(all_csv_files[x])"
      ],
      "metadata": {
        "id": "Hdawksf5UDZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the sorted wav list\n",
        "for x in range(10):\n",
        "  print(all_wav_files[x])"
      ],
      "metadata": {
        "id": "jx-Uc10DUEF3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for csv_file, wav_file in zip(all_csv_files, all_wav_files):\n",
        "\n",
        "    ## Create inputs ##\n",
        "    # Load the audio as a waveform y, store the sampling rate as sr\n",
        "    y, sr = librosa.load(wav_file)\n",
        "    # Create mel spectrogram\n",
        "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr)\n",
        "    # append spectrograms to spec_list\n",
        "    spectrogram = librosa.util.fix_length(data = spectrogram, size = 2048)\n",
        "    \n",
        "    # normalize values in spectrogram\n",
        "    # find the minimum and maximum values of the spectrogram\n",
        "    min_value = np.min(spectrogram)\n",
        "    max_value = np.max(spectrogram)\n",
        "    # normalize values to range [0,1]\n",
        "    normalized_spectrogram = np.apply_along_axis(lambda x: (x - min_value) / (max_value - min_value), 1, spectrogram)\n",
        "    # add a new axis for later network processing\n",
        "    normalized_spectrogram = normalized_spectrogram[:, :, newaxis]\n",
        "    # append spectrogram to spec_list\n",
        "    spec_list.append(normalized_spectrogram)\n",
        "\n",
        "    \n",
        "    n_augmented = 6  \n",
        "    # Data Augmentation\n",
        "    for i in range(n_augmented):\n",
        "      #noisy, sr = librosa.effects.add_noise(y,  SNR=20)     \n",
        "      pitch_shift = librosa.effects.pitch_shift(y = y, sr = sr,n_steps=i*2)\n",
        "      mel_spec = librosa.feature.melspectrogram(y=pitch_shift,sr=sr)\n",
        "      fix_len = librosa.util.fix_length(data = mel_spec, size = 2048)\n",
        "      min_value = np.min(fix_len)\n",
        "      max_value = np.max(fix_len)\n",
        "      # normalize values to range [0,1]\n",
        "      normalized_spectrogram = np.apply_along_axis(lambda x: (x - min_value) / (max_value - min_value), 1, fix_len)\n",
        "      normalized_spectrogram = np.expand_dims(normalized_spectrogram, axis = 2)\n",
        "      spec_list.append(normalized_spectrogram)\n",
        "\n",
        "\n",
        "    ## Create labels ##\n",
        "    # read in csv file\n",
        "    df = pd.read_csv(csv_file)\n",
        "    # convert csv file to numpy array\n",
        "    df_arr = df.to_numpy()\n",
        "    # create array with onset datapoints\n",
        "    df_onsets = df_arr[:, 0]\n",
        "    # create array with sound categories\n",
        "    df_vals = df_arr[:, 1]\n",
        "    # create list with time indices of onset times\n",
        "    time_indexes = []\n",
        "    # store onset times in the shape of our spectrogram\n",
        "    times = librosa.times_like(spectrogram)\n",
        "    # append each onset time into times_indexes list\n",
        "    for i in df_onsets:\n",
        "        time_indexes.append(np.abs(times - i).argmin())\n",
        "\n",
        "    # create blank target with the image-shape of our spectrogram\n",
        "    target = np.zeros((1,2048,4))\n",
        "\n",
        "    #print(df_vals, df_vals.shape)\n",
        "    #print(time_indexes)\n",
        "\n",
        "    # check the df_vals array for sound category for each onset\n",
        "\n",
        "    for i, val in enumerate(time_indexes):\n",
        "        if df_vals[i] == 'hhc':\n",
        "            # put in channel 1 if the onset is a closes hi-hat\n",
        "            target[:, val, 0] = 1.\n",
        "        elif df_vals[i] == 'hho':\n",
        "            #put in channel 2 if the onset is an open hi-hat\n",
        "            target[:, val, 1] = 1.\n",
        "        elif df_vals[i] == 'kd':\n",
        "            # put in channel 3 if the onset is a kick drum\n",
        "            target[:, val, 2] = 1.\n",
        "        elif df_vals[i] == 'sd':\n",
        "            # put in channel 4 if the onset is a snare drum\n",
        "            target[:, val, 3] = 1.\n",
        "\n",
        "    # np.set_printoptions(threshold=sys.maxsize)\n",
        "    # print(target[0,0:3,0:4])\n",
        "\n",
        "    # append targets to label_list\n",
        "    label_list.append(target)\n",
        "    for i in range(n_augmented):\n",
        "      label_list.append(target)\n",
        "      "
      ],
      "metadata": {
        "id": "UGvobnQvUL9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check lenght of input and label array\n",
        "print(len(spec_list))\n",
        "print(len(label_list))"
      ],
      "metadata": {
        "id": "PTpYAHU7UR35"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check one item for desired shape\n",
        "print(spec_list[0].shape, label_list[0].shape)"
      ],
      "metadata": {
        "id": "F6L-3eikUSdX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Save lists as tf datasets ##\n",
        "spec_ds = tf.data.Dataset.from_tensor_slices(spec_list)\n",
        "label_ds = tf.data.Dataset.from_tensor_slices(label_list)\n",
        "\n",
        "# zip together spec and label dataset\n",
        "dataset = tf.data.Dataset.zip((spec_ds, label_ds))\n",
        "# convert spectogram values to floats (normalization already performed)\n",
        "dataset = dataset.map(lambda spec, target: (spec, tf.cast(target, tf.float32)))\n",
        "# shuffle, batch, prefetch\n",
        "dataset_pre = dataset.shuffle(1000)\n",
        "dataset_pre = dataset_pre.batch(8)\n",
        "dataset_pre = dataset_pre.prefetch(20)\n",
        "\n",
        "print(dataset_pre)"
      ],
      "metadata": {
        "id": "ri4KkjKqUSwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for desired shapes\n",
        "for x,t in dataset_pre.take(1):\n",
        "    print(x.shape, t.shape)"
      ],
      "metadata": {
        "id": "MUYMC0kvUTCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the dataset with spectograms in the top row and the respective label in the bottom row\n",
        "n = 10\n",
        "fig, axs = plt.subplots(2, n, figsize=(50,10))\n",
        "\n",
        "for idx, (input, target) in enumerate(dataset):\n",
        "    if idx == n:\n",
        "        break\n",
        "\n",
        "    # display spectrogram\n",
        "    ax = axs[0, idx]\n",
        "    im = ax.imshow(librosa.power_to_db(input, ref=np.max), aspect='auto', origin='lower', interpolation='bicubic', cmap='jet')\n",
        "    ax.set_title(f\"Spectrogram_{idx}\")\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # display label\n",
        "    ax = axs[1, idx]\n",
        "    im = ax.imshow(tf.transpose(target), aspect='auto', origin='lower', interpolation='bicubic', cmap='jet')\n",
        "    ax.set_title(f\"Label_{idx}\")\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kG75uCq3UZ9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def conv_block(inputs, num_filters):\n",
        "  # run two convolutions with same padding, kernel size 3, relu activation and batchnorm\n",
        "  x = Conv2D(num_filters, 3, padding = 'same')(inputs)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  x = Conv2D(num_filters, 3, padding = 'same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "\n",
        "  return x"
      ],
      "metadata": {
        "id": "Bwbv2P4DUgEh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def encoder_block(inputs, num_filters):\n",
        "  x = conv_block(inputs, num_filters)\n",
        "  # MaxPooling(2,2) to reduce the size by half of the input array\n",
        "  p = MaxPool2D((2,2))(x)\n",
        "\n",
        "  return x, p"
      ],
      "metadata": {
        "id": "GkgFdbWTUiBB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def decoder_block(inputs, skip_features, num_filters):\n",
        "  # Up- convolution with strides = 2 in order to double the size of the input array and get the same size as the corresponding layer in the encoder\n",
        "  x = Conv2DTranspose(num_filters, (2,2), strides = 2, padding = 'same')(inputs)\n",
        "  # concatenate output with skip feature from the corresponding encoder layer of the same size\n",
        "  x = Concatenate()([x, skip_features])\n",
        "  # run classic convolution\n",
        "  x = conv_block(x, num_filters)\n",
        "  \n",
        "  return x"
      ],
      "metadata": {
        "id": "NLWOMkMHUkRA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_unet(input_shape):\n",
        "  inputs = Input(input_shape)\n",
        "\n",
        "  \"\"\"Encoder\"\"\"\n",
        "\n",
        "  # each encoder block returns one skip connection s (before pooling) and one pooling connection p for the contracting path\n",
        "  s1, p1 = encoder_block(inputs, 8) # 8 filters\n",
        "  s2, p2 = encoder_block(p1, 16) # 16 filters\n",
        "  s3, p3 = encoder_block(p2, 32) #32 filters\n",
        "  s4, p4 = encoder_block(p3, 64) # 64 filters\n",
        "\n",
        "  \"\"\"Bridge\"\"\"\n",
        "  # conv block without further pooling\n",
        "  b1 = conv_block(p4, 128)\n",
        "\n",
        "  \"\"\"Decoder\"\"\"\n",
        "\n",
        "  # each decoder block receives the input from the previous layer and the skip connection s from its corresponding encoder layer\n",
        "  d1 = decoder_block(b1, s4, 64) # 64 filters\n",
        "  d2 = decoder_block(d1, s3, 32) # 32 filters\n",
        "  d3 = decoder_block(d2, s2, 16) # 16 filters\n",
        "  d4 = decoder_block(d3, s1, 8) # 8 filters\n",
        "\n",
        "  \"\"\"Output\"\"\"\n",
        "  # Convolution with number of desired classes (4) as filters, kernelsize = 1 and softmax activation to normalize between [0,1]\n",
        "  # Now we have 4 matrices (one for each sound class) with the same size of the original input spectrogram\n",
        "  outputs = Conv2D(4, (1,1), padding = 'same', activation = 'softmax')(d4)\n",
        "\n",
        "  # as we're only interested in the onset times a 1D - array suffices for each category\n",
        "  # We reduce the vertical dimension with another conv layer of kernel size (128,1) to get four 1D arrays of the original lenght\n",
        "  outputs = Conv2D(4, (128,1), padding = 'valid', activation = 'sigmoid')(outputs)\n",
        "\n",
        "  model = Model(inputs, outputs, name = 'U-Net')\n",
        "  return model"
      ],
      "metadata": {
        "id": "eO2ONaNDUmyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (128,2048,1) \n",
        "input = dataset_pre\n",
        "model = build_unet(input_shape)\n",
        "model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['acc'])\n",
        "#model.summary()\n",
        "history = model.fit(input, epochs = 10)"
      ],
      "metadata": {
        "id": "XjXljlWjUoxd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}